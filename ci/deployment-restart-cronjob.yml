---
# Service account the client will use to reset the deployment,
# by default the pods running inside the cluster can do no such things.
kind: ServiceAccount
apiVersion: v1
metadata:
  name: k8-container-gate-restart
  namespace: default
---
# allow getting status and patching only the one deployment you want
# to restart
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8-container-gate-restart
  namespace: default
rules:
  - apiGroups: ["apps", "extensions"]
    resources: ["deployments"]
    resourceNames: ["k8-container-gate"]
    verbs: ["get", "patch", "list", "watch"]
---
# bind the role to the service account
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8-container-gate-restart
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: k8-container-gate-restart
subjects:
  - kind: ServiceAccount
    name: k8-container-gate-restart
    namespace: default
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: k8-container-gate-restart
  namespace: default
spec:
  concurrencyPolicy: Forbid
  schedule: '0 3 */3 * *'  # Every 3 days at 3 AM
  jobTemplate:
    spec:
      backoffLimit: 2
      template:
        spec:
          serviceAccountName: k8-container-gate-restart
          containers:
          - name: kubectl
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - kubectl rollout restart deployment/k8-container-gate
          restartPolicy: OnFailure